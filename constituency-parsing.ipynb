{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk as nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enabling eager execution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Glove vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1783088, 11)\n",
      "    0        1         2         3         4         5         6       7   \\\n",
      "0    ,  0.18378 -0.121230 -0.119870  0.015227 -0.191210 -0.066074 -2.9876   \n",
      "1  the -0.20838 -0.149320 -0.017528 -0.028432 -0.060104 -0.264600 -4.1445   \n",
      "2    .  0.10876  0.002244  0.222130 -0.121020 -0.048959  0.018135 -3.8174   \n",
      "3  and -0.09611 -0.257880 -0.358600 -0.328870  0.579500 -0.517740 -4.1582   \n",
      "4   to -0.24837 -0.454610  0.039227 -0.284220 -0.031852  0.263550 -4.6323   \n",
      "\n",
      "         8         9         10  \n",
      "0  0.807950  0.067338 -0.131840  \n",
      "1  0.629320  0.336720 -0.433950  \n",
      "2 -0.032631 -0.625940 -0.518980  \n",
      "3 -0.113710 -0.108480 -0.488850  \n",
      "4  0.013890 -0.539280 -0.084454  \n"
     ]
    }
   ],
   "source": [
    "UNK = '<unk>'\n",
    "\n",
    "glovePath = '/media/kandy/hdd/master-thesis/datasets/'\n",
    "outfile = glovePath +'glove_word_corpus.pic'\n",
    "\n",
    "with open(outfile, 'rb') as pickle_file:    \n",
    "    gloveCorpus, glove_corpus_word_to_int, glove_corpus_int_to_word = pickle.load(pickle_file)\n",
    "gloveSet = pd.read_csv(glovePath+'glove.42B.10d.txt', sep=' ', header=None)\n",
    "print(gloveSet.shape)\n",
    "print(gloveSet.head())\n",
    "\n",
    "gloveWords = gloveSet.iloc[:,0:1]\n",
    "gloveVectors = gloveSet.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WSJ.txt']\n",
      "<class 'nltk.corpus.reader.bracket_parse.BracketParseCorpusReader'>\n",
      "No. of trees:  49208\n",
      "<class 'nltk.tree.Tree'>\n"
     ]
    }
   ],
   "source": [
    "## https://www.nltk.org/_modules/nltk/tree.html\n",
    "## above link contains the API and also some tutorials\n",
    "\n",
    "#reader = nltk.corpus.BracketParseCorpusReader('.','SWB-all-sentences-original-with-punctuation.MRG')\n",
    "reader = nltk.corpus.BracketParseCorpusReader('.','WSJ.txt')\n",
    "print(reader.fileids())\n",
    "print(type(reader))\n",
    "\n",
    "## reads the file and converts each line into a tree\n",
    "trees = reader.parsed_sents()\n",
    "print('No. of trees: ', len(trees))\n",
    "print(type(trees[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>tree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Pierre, Vinken, ,, 61, years, old, ,, will, j...</td>\n",
       "      <td>[[[(NP (NNP Pierre) (NNP Vinken)), (, ,), (ADJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Mr., Vinken, is, chairman, of, Elsevier, N.V....</td>\n",
       "      <td>[[[(NNP Mr.), (NNP Vinken)], [(VBZ is), (NP-PR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Rudolph, Agnew, ,, 55, years, old, and, forme...</td>\n",
       "      <td>[[[(NP (NNP Rudolph) (NNP Agnew)), (, ,), (UCP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[A, form, of, asbestos, once, used, *, *, to, ...</td>\n",
       "      <td>[[[(NP-SBJ\\n  (NP (NP (DT A) (NN form)) (PP (I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The, asbestos, fiber, ,, crocidolite, ,, is, ...</td>\n",
       "      <td>[[[(NP-SBJ\\n  (NP (DT The) (NN asbestos) (NN f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  [Pierre, Vinken, ,, 61, years, old, ,, will, j...   \n",
       "1  [Mr., Vinken, is, chairman, of, Elsevier, N.V....   \n",
       "2  [Rudolph, Agnew, ,, 55, years, old, and, forme...   \n",
       "3  [A, form, of, asbestos, once, used, *, *, to, ...   \n",
       "4  [The, asbestos, fiber, ,, crocidolite, ,, is, ...   \n",
       "\n",
       "                                                tree  \n",
       "0  [[[(NP (NNP Pierre) (NNP Vinken)), (, ,), (ADJ...  \n",
       "1  [[[(NNP Mr.), (NNP Vinken)], [(VBZ is), (NP-PR...  \n",
       "2  [[[(NP (NNP Rudolph) (NNP Agnew)), (, ,), (UCP...  \n",
       "3  [[[(NP-SBJ\\n  (NP (NP (DT A) (NN form)) (PP (I...  \n",
       "4  [[[(NP-SBJ\\n  (NP (DT The) (NN asbestos) (NN f...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treesDict = [{'sentence': tree.leaves(), 'tree':tree} for tree in trees]\n",
    "treeDataframe = pd.DataFrame(data=treesDict, columns=['sentence', 'tree'])\n",
    "treeDataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>tree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1127477, 1127477, 691001, 1127477, 1127477, 1...</td>\n",
       "      <td>[[[(NP (NNP Pierre) (NNP Vinken)), (, ,), (ADJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1127477, 1127477, 1127477, 1127477, 1264798, ...</td>\n",
       "      <td>[[[(NNP Mr.), (NNP Vinken)], [(VBZ is), (NP-PR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1127477, 1127477, 691001, 1127477, 1127477, 1...</td>\n",
       "      <td>[[[(NP (NNP Rudolph) (NNP Agnew)), (, ,), (UCP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1127477, 1127477, 1264798, 1127477, 1127477, ...</td>\n",
       "      <td>[[[(NP-SBJ\\n  (NP (NP (DT A) (NN form)) (PP (I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1127477, 1127477, 1127477, 691001, 211388, 69...</td>\n",
       "      <td>[[[(NP-SBJ\\n  (NP (DT The) (NN asbestos) (NN f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  [1127477, 1127477, 691001, 1127477, 1127477, 1...   \n",
       "1  [1127477, 1127477, 1127477, 1127477, 1264798, ...   \n",
       "2  [1127477, 1127477, 691001, 1127477, 1127477, 1...   \n",
       "3  [1127477, 1127477, 1264798, 1127477, 1127477, ...   \n",
       "4  [1127477, 1127477, 1127477, 691001, 211388, 69...   \n",
       "\n",
       "                                                tree  \n",
       "0  [[[(NP (NNP Pierre) (NNP Vinken)), (, ,), (ADJ...  \n",
       "1  [[[(NNP Mr.), (NNP Vinken)], [(VBZ is), (NP-PR...  \n",
       "2  [[[(NP (NNP Rudolph) (NNP Agnew)), (, ,), (UCP...  \n",
       "3  [[[(NP-SBJ\\n  (NP (NP (DT A) (NN form)) (PP (I...  \n",
       "4  [[[(NP-SBJ\\n  (NP (DT The) (NN asbestos) (NN f...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_imdb_corpus_into_int(words):\n",
    "    words = [word if word in gloveCorpus else UNK for word in words]\n",
    "    words_to_num = [glove_corpus_word_to_int[word] for word in words]\n",
    "    return words_to_num\n",
    "\n",
    "treeDataframe_num = treeDataframe.copy()\n",
    "treeDataframe_num['sentence'] = treeDataframe_num['sentence'].apply(convert_imdb_corpus_into_int)\n",
    "treeDataframe_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147, 2)\n",
      "(49061, 2)\n"
     ]
    }
   ],
   "source": [
    "treeDF_train, treeDF_test = sklearn.model_selection.train_test_split(treeDataframe_num, test_size=0.997)\n",
    "print(treeDF_train.shape)\n",
    "print(treeDF_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SIZE = 10\n",
    "embeddings = tfe.Variable(name='embeddings', validate_shape= gloveVectors.shape, \n",
    "                          initial_value=gloveVectors.values, \n",
    "                          dtype=tf.float32, trainable=False)\n",
    "w = tfe.Variable(name='w', validate_shape=(2*gloveVectors.shape[1], STATE_SIZE), \n",
    "                 initial_value=0.01 * tf.random_normal(shape=(2*gloveVectors.shape[1], STATE_SIZE)),\n",
    "                 dtype=tf.float32)\n",
    "b = tfe.Variable(name='b', validate_shape=(1, STATE_SIZE),\n",
    "                 initial_value=0.01 * tf.random_normal(shape=(1, STATE_SIZE)),\n",
    "                 dtype=tf.float32)\n",
    "\n",
    "w_score = tfe.Variable(name='w_score', validate_shape=(STATE_SIZE, 1), \n",
    "                 initial_value=0.01 * tf.random_normal(shape=(STATE_SIZE, 1)),\n",
    "                 dtype=tf.float32)\n",
    "b_score = tfe.Variable(name='b_score', validate_shape=(1, 1),\n",
    "                 initial_value=0.01 * tf.random_normal(shape=(1, 1)),\n",
    "                 dtype=tf.float32)\n",
    "\n",
    "#print(w)\n",
    "#print(b)\n",
    "#print(w_score)\n",
    "#print(b_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1783088, 10)\n",
      "tf.Tensor(\n",
      "[[ 0.32699   0.17616   0.47762   0.15523  -0.43263   0.044493  0.40132\n",
      "  -0.46666   0.28362   0.88024 ]], shape=(1, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(embeddings.shape)\n",
    "lookup = tf.nn.embedding_lookup(embeddings, glove_corpus_word_to_int['<unk>'])\n",
    "lookup = tf.reshape(lookup, shape=(1, lookup.shape[0]))\n",
    "print(lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Critics', 'of', 'the', 'present', 'arrangement', 'are', 'correct', '*-1', 'to', 'say', 'that', 'it', 'is', 'undemocratic', '.']\n",
      "tf.Tensor(-0.06728338, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def compute_score_for_tree(tree, weights, embeddings, scores):\n",
    "    if(type(tree[0]) == type('a string')):\n",
    "        #print(tree.label() + ' : ' + tree[0])\n",
    "        word = tree[0].lower()\n",
    "        if(word not in glove_corpus_word_to_int):\n",
    "            word = '<unk>'\n",
    "        word_vector = tf.nn.embedding_lookup(embeddings, glove_corpus_word_to_int[word])\n",
    "        word_vector = tf.reshape(word_vector, shape=(1, word_vector.shape[0]))\n",
    "        return word_vector\n",
    "        #print('depth is reached !!!')\n",
    "        #return\n",
    "\n",
    "    #for i in range(len(tree)):\n",
    "    #    print('Inside tree : '+ tree[i].label())\n",
    "    #    compute_score_for_tree(tree[i], weights, embeddings)\n",
    "\n",
    "\n",
    "        \n",
    "    left = compute_score_for_tree(tree[0], weights, embeddings, scores)\n",
    "    if(len(tree) !=2):\n",
    "        return left\n",
    "\n",
    "    right = compute_score_for_tree(tree[1], weights, embeddings, scores)\n",
    "    words_concat = tf.concat([left, right], axis=0)\n",
    "    #print(words_concat.shape)\n",
    "    #print(left.shape, right.shape)\n",
    "    words_concat = tf.reshape(words_concat, shape=(1, left.shape[1] + right.shape[1]))\n",
    "    #print(words_concat)\n",
    "    z = tf.matmul(words_concat, weights[0]) + weights[1]\n",
    "    state_vec = tf.tanh(z)\n",
    "    \n",
    "    #print(state_vec)\n",
    "    score = tf.matmul(state_vec, weights[2]) + weights[3]\n",
    "    scores.append(score)\n",
    "    return state_vec\n",
    "    \n",
    "    \n",
    "for j in range(treeDF_train.shape[0]):\n",
    "    tree = treeDF_train.iat[j,1]\n",
    "    print(tree.leaves())\n",
    "    scores = []\n",
    "    tree.chomsky_normal_form()\n",
    "    compute_score_for_tree(tree[0], [w,b,w_score,b_score], embeddings, scores)\n",
    "    #print(sum(scores))\n",
    "    total_score = tf.reduce_sum(tf.stack(scores))\n",
    "    print(total_score)\n",
    "    break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 10)\n",
      "(10,)\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "epoch = 1\n",
    "\n",
    "for i in range(epoch):\n",
    "    for j in range(treeDF_train.shape[0]):\n",
    "        with tf.GradientTape() as tape:\n",
    "            words = tf.nn.embedding_lookup(embeddings, treeDF_train.iat[j,0])\n",
    "            numpy_words = words.numpy()\n",
    "            tensor_words =  tf.convert_to_tensor(numpy_words)\n",
    "            words_len = tensor_words.shape[0]\n",
    "            total_pred_score = 0\n",
    "            print(words.shape)\n",
    "            words_unstack = tf.unstack(words)\n",
    "            print(words_unstack[0].shape)\n",
    "            print(type(words_unstack))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0041068792\n",
      "0.001674056\n",
      "0.00043857098\n",
      "0.0007224083\n",
      "0.0011472702\n",
      "0.0011411905\n",
      "0.00085377693\n",
      "0.00034725666\n",
      "0.00035119057\n",
      "0.0005912781\n",
      "0.0006707907\n",
      "0.00054585934\n",
      "0.00020194054\n",
      "0.0002450943\n",
      "0.00049591064\n",
      "0.0006041527\n",
      "0.0006004572\n",
      "0.0005013943\n",
      "0.004415989\n",
      "0.0042316914\n",
      "0.003971815\n",
      "0.0037435293\n",
      "0.0032821894\n",
      "0.0027462244\n",
      "0.0020823479\n",
      "0.00038921833\n",
      "0.00055754185\n",
      "0.0011968613\n",
      "0.0015074015\n",
      "0.00149858\n",
      "0.0014225245\n",
      "0.00087058544\n",
      "0.0013151169\n",
      "0.0018045902\n",
      "0.0020039082\n",
      "0.0019580126\n",
      "0.0016981363\n",
      "0.0012476444\n",
      "0.00090551376\n",
      "0.0012358427\n",
      "0.0013753176\n",
      "0.0013167858\n",
      "0.0005239248\n",
      "0.0005823374\n",
      "0.00044322014\n",
      "0.00012338161\n",
      "0.0019847155\n",
      "0.001956582\n",
      "0.0019820929\n",
      "0.0020834208\n",
      "2.5987625e-05\n",
      "0.000213027\n",
      "0.0004886389\n",
      "0.0005363226\n",
      "0.0003772974\n",
      "2.0742416e-05\n",
      "0.0005322695\n",
      "0.00078749657\n",
      "0.0007582903\n",
      "0.0004723072\n",
      "0.00014078617\n",
      "0.00035250187\n",
      "0.00029456615\n",
      "1.0251999e-05\n",
      "2.1457672e-05\n",
      "0.00023388863\n",
      "0.00020122528\n",
      "9.274483e-05\n",
      "8.404255e-05\n",
      "0.00019693375\n",
      "0.00018048286\n",
      "0.00010383129\n",
      "8.273125e-05\n",
      "0.00021135807\n",
      "0.00020515919\n",
      "7.021427e-05\n",
      "4.1604042e-05\n",
      "0.00025737286\n",
      "0.00025761127\n",
      "9.179115e-06\n",
      "2.3126602e-05\n",
      "0.00021958351\n",
      "0.00016129017\n",
      "0.00016343594\n",
      "0.00018835068\n",
      "5.3286552e-05\n",
      "2.3841858e-06\n",
      "0.00030767918\n",
      "0.0003273487\n",
      "8.881092e-05\n",
      "0.0003902912\n",
      "0.00055253506\n",
      "0.00042390823\n",
      "4.1484833e-05\n",
      "0.00055098534\n",
      "0.0008337498\n",
      "0.0008531809\n",
      "0.0006381273\n",
      "0.00020503998\n",
      "0.0004401207\n",
      "Time taken to execute (seconds):  3899.8696323490003\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "epoch = 100\n",
    "\n",
    "for i in range(epoch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        for j in range(treeDF_train.shape[0]):\n",
    "            words = tf.nn.embedding_lookup(embeddings, treeDF_train.iat[j,0])\n",
    "            words_unstack = tf.unstack(words)\n",
    "            words_len = len(words_unstack)\n",
    "            pred_score_list = []\n",
    "            total_loss_list = []\n",
    "            while(words_len > 1):\n",
    "                #print(words_len)\n",
    "                scores = np.zeros(shape=(words_len-1, 1))\n",
    "                for k in range(words_len - 1):\n",
    "                    words_concat = tf.concat([words_unstack[k], words_unstack[k+1]], axis=0)\n",
    "                    words_concat = tf.reshape(words_concat, shape=(1, words_concat.shape[0]))\n",
    "                    #print(words_concat)\n",
    "                    z = tf.matmul(words_concat, w) + b\n",
    "                    state_vec = tf.tanh(z)\n",
    "                    #print(state_vec)\n",
    "                    score = tf.matmul(state_vec, w_score) + b_score\n",
    "                    #print(score)\n",
    "                    scores[k] = score\n",
    "                \n",
    "                #print(scores)\n",
    "                \n",
    "                max_score_index = np.argmax(scores) \n",
    "                pred_score_list.append(scores[max_score_index])\n",
    "                #print(max_score_index)\n",
    "                words_unstack.pop(max_score_index+1)\n",
    "                #print(len(words_unstack))\n",
    "                words_unstack.pop(max_score_index)\n",
    "                #print(len(words_unstack))\n",
    "                state_vec_vector = tf.reshape(state_vec, shape = [state_vec.shape[1]])\n",
    "                #print(state_vec_vector)\n",
    "                words_unstack.insert(max_score_index, state_vec_vector)\n",
    "                #print(len(words_unstack))\n",
    "                words_len = len(words_unstack)\n",
    "                #print(words_len)\n",
    "            \n",
    "            act_score_list = []\n",
    "            tree = treeDF_train.iat[j,1]\n",
    "            tree.chomsky_normal_form()\n",
    "            compute_score_for_tree(tree[0], [w,b,w_score,b_score], embeddings, act_score_list)\n",
    "            total_act_score = tf.reduce_sum(tf.stack(act_score_list))\n",
    "            total_pred_score = tf.reduce_sum(tf.stack(pred_score_list))\n",
    "            #print(total_act_score)\n",
    "            #print(total_pred_score)\n",
    "            loss = tf.losses.absolute_difference(total_act_score,  total_pred_score)\n",
    "            total_loss_list.append(loss)\n",
    "            #loss = tf.losses.sigmoid_cross_entropy(tf.constant(imdb_train.iat[j,1], shape=(1,1)), y_predict)\n",
    "        \n",
    "        total_loss = tf.reduce_mean(tf.stack(total_loss_list))\n",
    "        grads = tape.gradient(total_loss, [w,b,w_score,b_score])\n",
    "        #print(grads)\n",
    "        optimizer.apply_gradients(zip(grads, [w,b,w_score,b_score]))\n",
    "            #,global_step=tf.train.get_or_create_global_step())\n",
    "        print(total_loss.numpy())\n",
    "        #if(j == 100):\n",
    "        #    break\n",
    "        \n",
    "end = timer()\n",
    "print('Time taken to execute (seconds): ', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: [\"<tf.Variable 'w_score:0' shape=(10, 1) dtype=float32, numpy=\\narray([[-0.00154085],\\n       [-0.00365236],\\n       [-0.00598969],\\n       [-0.00382886],\\n       [-0.00333749],\\n       [-0.01275734],\\n       [-0.00310688],\\n       [ 0.00849401],\\n       [-0.01227861],\\n       [ 0.00580805]], dtype=float32)>\", \"<tf.Variable 'b_score:0' shape=(1, 1) dtype=float32, numpy=array([[-0.00917668]], dtype=float32)>\", \"<tf.Variable 'w:0' shape=(20, 10) dtype=float32, numpy=\\narray([[-3.09174345e-03, -5.74955996e-03, -8.96898750e-03,\\n        -1.94976723e-03,  7.19460612e-03,  2.89615733e-03,\\n         1.01199206e-02, -3.93189816e-03, -2.19476898e-03,\\n         3.18261143e-03],\\n       [ 6.97274320e-03, -1.18933683e-02,  1.33814020e-02,\\n        -2.06305552e-03, -1.10047432e-02, -7.98548106e-03,\\n        -1.16529735e-02,  1.74845587e-02,  2.48650163e-02,\\n         6.07878016e-03],\\n       [ 1.39661517e-03,  7.68934609e-03,  2.68776016e-03,\\n         9.95987374e-03, -9.35456264e-05,  2.17868388e-03,\\n         1.17401704e-02, -1.08868750e-02, -1.08957225e-02,\\n         9.17153992e-03],\\n       [ 9.02451109e-03,  4.45182202e-03, -1.71463809e-03,\\n         1.89139247e-02, -1.92416157e-03,  7.12427171e-03,\\n         4.11842950e-03,  3.45513923e-03, -1.73334926e-02,\\n        -7.75449723e-03],\\n       [ 9.80109069e-03, -3.49491648e-03, -9.11852904e-03,\\n        -2.96191592e-03, -8.08261894e-03, -1.27005740e-03,\\n         1.84484366e-02,  5.25826705e-04, -1.84372312e-03,\\n        -4.00816835e-03],\\n       [ 6.22068346e-03, -9.17762611e-03, -7.88632687e-03,\\n         1.69579592e-02, -6.41970756e-03, -7.45966379e-03,\\n        -5.77369751e-03,  8.18815362e-03,  5.83289796e-03,\\n         1.20820124e-02],\\n       [-1.71516847e-03, -4.80221258e-03,  1.15765231e-02,\\n        -1.94087531e-02,  1.33954585e-02,  2.16873679e-02,\\n        -3.94105166e-03,  8.04960262e-03,  4.16671811e-03,\\n         2.51371018e-03],\\n       [-1.40991611e-02, -1.58825740e-02, -3.45084397e-03,\\n         2.25161738e-03,  4.35629045e-04,  1.04212295e-02,\\n        -3.74808605e-03, -6.13378966e-03, -6.73765689e-03,\\n         1.15719493e-02],\\n       [ 1.63916475e-03,  1.22696729e-02,  7.22868554e-03,\\n        -3.61649832e-03, -7.15096598e-04, -1.96413789e-02,\\n         3.28121497e-03,  2.46137660e-03, -8.47877376e-03,\\n        -9.32355691e-03],\\n       [-1.19181173e-02,  1.07893227e-02,  5.73976571e-03,\\n        -5.03425067e-03, -1.06739663e-02,  2.04937141e-02,\\n         8.84764269e-03,  3.54238180e-03, -1.35535365e-02,\\n        -9.74597968e-03],\\n       [ 7.00888271e-03,  1.12532098e-02, -2.88254116e-03,\\n         1.02232788e-02, -7.25333951e-03, -3.40706203e-03,\\n        -1.36010228e-02, -1.50938686e-02, -1.28285750e-03,\\n        -2.89220852e-03],\\n       [-2.50755996e-02,  4.53507900e-03, -5.99443121e-03,\\n        -8.10758211e-04,  1.18189678e-02,  3.52143007e-03,\\n        -1.28063874e-03, -1.88804660e-02, -9.97012202e-03,\\n        -1.90508121e-03],\\n       [-1.19528351e-02, -8.62946175e-03,  2.90089771e-02,\\n        -2.12417814e-04,  1.41580701e-02,  2.84806117e-02,\\n        -2.95294099e-03, -1.29318424e-02, -4.01881896e-03,\\n         1.15521462e-03],\\n       [ 6.31020730e-03, -4.15504415e-04,  1.45845590e-02,\\n         2.57285847e-03,  6.05166424e-03,  2.36604875e-03,\\n        -7.08058337e-03, -7.07692560e-03, -6.68112002e-03,\\n        -2.58101127e-03],\\n       [ 1.95625667e-02, -7.05379294e-03,  8.14766996e-03,\\n        -3.15949204e-04, -4.99683386e-03,  4.17695707e-03,\\n        -4.73600207e-03,  1.16216801e-02, -3.48707964e-03,\\n        -7.68754771e-03],\\n       [ 1.39405327e-02,  7.72135891e-03, -3.65331024e-03,\\n        -3.97247914e-03, -1.46261947e-02,  6.98104408e-03,\\n        -5.41888503e-03,  1.69727225e-02,  1.17874993e-02,\\n        -1.50906695e-02],\\n       [ 6.25656964e-03, -6.79577049e-03, -6.43751072e-03,\\n        -1.09023303e-02, -4.69371211e-03,  4.82316082e-03,\\n        -8.20855424e-03, -7.38050323e-03, -8.35179165e-03,\\n        -3.48199811e-03],\\n       [ 9.05214995e-03,  1.27361333e-02, -1.83792543e-02,\\n         2.00365614e-02,  3.06330854e-03, -9.39836353e-03,\\n         9.20790527e-03,  5.43366233e-03, -2.07816325e-02,\\n         5.02717588e-03],\\n       [-5.50827850e-03, -5.71877928e-03, -2.77381227e-03,\\n        -6.92338031e-03,  1.92580407e-03,  1.04830787e-03,\\n        -5.97799756e-03, -1.04356753e-02, -1.30497562e-02,\\n         2.12278147e-03],\\n       [ 1.76440403e-02, -6.97733974e-03, -1.52996611e-02,\\n         1.35736819e-03,  6.36868572e-05,  9.06752609e-03,\\n        -3.31368973e-03,  1.10831326e-02,  8.87572579e-03,\\n        -6.81262882e-03]], dtype=float32)>\", \"<tf.Variable 'b:0' shape=(1, 10) dtype=float32, numpy=\\narray([[ 0.00263612, -0.01090427,  0.01265588,  0.01441635, -0.01406689,\\n        -0.00690599, -0.00608754,  0.00096349,  0.00586474, -0.00322387]],\\n      dtype=float32)>\"].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-36f013ee12ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;31m#print(grads)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0;31m#,global_step=tf.train.get_or_create_global_step())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp-master-thesis/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m       raise ValueError(\"No gradients provided for any variable: %s.\" %\n\u001b[0;32m--> 591\u001b[0;31m                        ([str(v) for _, v, _ in converted_grads_and_vars],))\n\u001b[0m\u001b[1;32m    592\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_slots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable: [\"<tf.Variable 'w_score:0' shape=(10, 1) dtype=float32, numpy=\\narray([[-0.00154085],\\n       [-0.00365236],\\n       [-0.00598969],\\n       [-0.00382886],\\n       [-0.00333749],\\n       [-0.01275734],\\n       [-0.00310688],\\n       [ 0.00849401],\\n       [-0.01227861],\\n       [ 0.00580805]], dtype=float32)>\", \"<tf.Variable 'b_score:0' shape=(1, 1) dtype=float32, numpy=array([[-0.00917668]], dtype=float32)>\", \"<tf.Variable 'w:0' shape=(20, 10) dtype=float32, numpy=\\narray([[-3.09174345e-03, -5.74955996e-03, -8.96898750e-03,\\n        -1.94976723e-03,  7.19460612e-03,  2.89615733e-03,\\n         1.01199206e-02, -3.93189816e-03, -2.19476898e-03,\\n         3.18261143e-03],\\n       [ 6.97274320e-03, -1.18933683e-02,  1.33814020e-02,\\n        -2.06305552e-03, -1.10047432e-02, -7.98548106e-03,\\n        -1.16529735e-02,  1.74845587e-02,  2.48650163e-02,\\n         6.07878016e-03],\\n       [ 1.39661517e-03,  7.68934609e-03,  2.68776016e-03,\\n         9.95987374e-03, -9.35456264e-05,  2.17868388e-03,\\n         1.17401704e-02, -1.08868750e-02, -1.08957225e-02,\\n         9.17153992e-03],\\n       [ 9.02451109e-03,  4.45182202e-03, -1.71463809e-03,\\n         1.89139247e-02, -1.92416157e-03,  7.12427171e-03,\\n         4.11842950e-03,  3.45513923e-03, -1.73334926e-02,\\n        -7.75449723e-03],\\n       [ 9.80109069e-03, -3.49491648e-03, -9.11852904e-03,\\n        -2.96191592e-03, -8.08261894e-03, -1.27005740e-03,\\n         1.84484366e-02,  5.25826705e-04, -1.84372312e-03,\\n        -4.00816835e-03],\\n       [ 6.22068346e-03, -9.17762611e-03, -7.88632687e-03,\\n         1.69579592e-02, -6.41970756e-03, -7.45966379e-03,\\n        -5.77369751e-03,  8.18815362e-03,  5.83289796e-03,\\n         1.20820124e-02],\\n       [-1.71516847e-03, -4.80221258e-03,  1.15765231e-02,\\n        -1.94087531e-02,  1.33954585e-02,  2.16873679e-02,\\n        -3.94105166e-03,  8.04960262e-03,  4.16671811e-03,\\n         2.51371018e-03],\\n       [-1.40991611e-02, -1.58825740e-02, -3.45084397e-03,\\n         2.25161738e-03,  4.35629045e-04,  1.04212295e-02,\\n        -3.74808605e-03, -6.13378966e-03, -6.73765689e-03,\\n         1.15719493e-02],\\n       [ 1.63916475e-03,  1.22696729e-02,  7.22868554e-03,\\n        -3.61649832e-03, -7.15096598e-04, -1.96413789e-02,\\n         3.28121497e-03,  2.46137660e-03, -8.47877376e-03,\\n        -9.32355691e-03],\\n       [-1.19181173e-02,  1.07893227e-02,  5.73976571e-03,\\n        -5.03425067e-03, -1.06739663e-02,  2.04937141e-02,\\n         8.84764269e-03,  3.54238180e-03, -1.35535365e-02,\\n        -9.74597968e-03],\\n       [ 7.00888271e-03,  1.12532098e-02, -2.88254116e-03,\\n         1.02232788e-02, -7.25333951e-03, -3.40706203e-03,\\n        -1.36010228e-02, -1.50938686e-02, -1.28285750e-03,\\n        -2.89220852e-03],\\n       [-2.50755996e-02,  4.53507900e-03, -5.99443121e-03,\\n        -8.10758211e-04,  1.18189678e-02,  3.52143007e-03,\\n        -1.28063874e-03, -1.88804660e-02, -9.97012202e-03,\\n        -1.90508121e-03],\\n       [-1.19528351e-02, -8.62946175e-03,  2.90089771e-02,\\n        -2.12417814e-04,  1.41580701e-02,  2.84806117e-02,\\n        -2.95294099e-03, -1.29318424e-02, -4.01881896e-03,\\n         1.15521462e-03],\\n       [ 6.31020730e-03, -4.15504415e-04,  1.45845590e-02,\\n         2.57285847e-03,  6.05166424e-03,  2.36604875e-03,\\n        -7.08058337e-03, -7.07692560e-03, -6.68112002e-03,\\n        -2.58101127e-03],\\n       [ 1.95625667e-02, -7.05379294e-03,  8.14766996e-03,\\n        -3.15949204e-04, -4.99683386e-03,  4.17695707e-03,\\n        -4.73600207e-03,  1.16216801e-02, -3.48707964e-03,\\n        -7.68754771e-03],\\n       [ 1.39405327e-02,  7.72135891e-03, -3.65331024e-03,\\n        -3.97247914e-03, -1.46261947e-02,  6.98104408e-03,\\n        -5.41888503e-03,  1.69727225e-02,  1.17874993e-02,\\n        -1.50906695e-02],\\n       [ 6.25656964e-03, -6.79577049e-03, -6.43751072e-03,\\n        -1.09023303e-02, -4.69371211e-03,  4.82316082e-03,\\n        -8.20855424e-03, -7.38050323e-03, -8.35179165e-03,\\n        -3.48199811e-03],\\n       [ 9.05214995e-03,  1.27361333e-02, -1.83792543e-02,\\n         2.00365614e-02,  3.06330854e-03, -9.39836353e-03,\\n         9.20790527e-03,  5.43366233e-03, -2.07816325e-02,\\n         5.02717588e-03],\\n       [-5.50827850e-03, -5.71877928e-03, -2.77381227e-03,\\n        -6.92338031e-03,  1.92580407e-03,  1.04830787e-03,\\n        -5.97799756e-03, -1.04356753e-02, -1.30497562e-02,\\n         2.12278147e-03],\\n       [ 1.76440403e-02, -6.97733974e-03, -1.52996611e-02,\\n         1.35736819e-03,  6.36868572e-05,  9.06752609e-03,\\n        -3.31368973e-03,  1.10831326e-02,  8.87572579e-03,\\n        -6.81262882e-03]], dtype=float32)>\", \"<tf.Variable 'b:0' shape=(1, 10) dtype=float32, numpy=\\narray([[ 0.00263612, -0.01090427,  0.01265588,  0.01441635, -0.01406689,\\n        -0.00690599, -0.00608754,  0.00096349,  0.00586474, -0.00322387]],\\n      dtype=float32)>\"]."
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "epoch = 1\n",
    "\n",
    "for i in range(epoch):\n",
    "    for j in range(treeDF_train.shape[0]):\n",
    "        with tf.GradientTape() as tape:\n",
    "            words = tf.nn.embedding_lookup(embeddings, treeDF_train.iat[j,0])\n",
    "            numpy_words = words.numpy()\n",
    "            tensor_words =  tf.convert_to_tensor(numpy_words)\n",
    "            words_len = tensor_words.shape[0]\n",
    "            total_pred_score = 0\n",
    "            #print(words_len)\n",
    "            while(words_len > 1):\n",
    "                scores = np.zeros(shape=(words_len-1, 1))\n",
    "                for k in range(words_len - 1):\n",
    "                    words_concat = tf.concat([tensor_words[k], tensor_words[k+1]], axis=0)\n",
    "                    words_concat = tf.reshape(words_concat, shape=(1, words_concat.shape[0]))\n",
    "                    #print(words_concat)\n",
    "                    z = tf.matmul(words_concat, w) + b\n",
    "                    state_vec = tf.tanh(z)\n",
    "                    #print(state_vec)\n",
    "                    score = tf.matmul(state_vec, w_score) + b_score\n",
    "                    #print(score)\n",
    "                    scores[k] = score\n",
    "                \n",
    "                #print(scores)\n",
    "                \n",
    "                max_score_index = np.argmax(scores) \n",
    "                total_pred_score = total_pred_score + scores[max_score_index]\n",
    "                #print(max_score_index)\n",
    "                list_words = numpy_words.tolist()\n",
    "                #print(len(list_words))\n",
    "                list_words.pop(max_score_index+1)\n",
    "                #print(len(list_words))\n",
    "                list_words.pop(max_score_index)\n",
    "                #print(len(list_words))\n",
    "                list_words.insert(max_score_index, state_vec.numpy().tolist()[0])\n",
    "                #print(len(list_words))\n",
    "                numpy_words = np.array(list_words)\n",
    "                #print(numpy_words)\n",
    "                #print(numpy_words.shape)\n",
    "                tensor_words =  tf.convert_to_tensor(list_words)\n",
    "                words_len = tensor_words.shape[0]\n",
    "                #print(words_len)\n",
    "            \n",
    "            scores = []\n",
    "            tree = treeDF_train.iat[j,1]\n",
    "            tree.chomsky_normal_form()\n",
    "            compute_score_for_tree(tree[0], [w,b,w_score,b_score], embeddings, scores)\n",
    "            total_actual_score = sum(scores)\n",
    "            #print(type(total_actual_score))\n",
    "            #print(type(total_pred_score))\n",
    "            loss = tf.losses.absolute_difference(total_actual_score,  total_pred_score[0])\n",
    "            \n",
    "            #loss = tf.losses.sigmoid_cross_entropy(tf.constant(imdb_train.iat[j,1], shape=(1,1)), y_predict)\n",
    "           \n",
    "            grads = tape.gradient(loss, [w_score, b_score, w, b])\n",
    "            #print(grads)\n",
    "            optimizer.apply_gradients(zip(grads, [w_score, b_score, w, b]))\n",
    "            #,global_step=tf.train.get_or_create_global_step())\n",
    "        if(j == 100):\n",
    "            break\n",
    "        #print(loss.numpy())\n",
    "        \n",
    "end = timer()\n",
    "print('Time taken to execute (seconds): ', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "45\n",
      "[[0.01782897114753723, 0.001361352507956326, -0.005683459807187319, -0.002537204185500741, 0.03020063415169716, -0.0396573543548584, -0.01406009029597044, -0.004425551276654005, 0.0037269028834998608, 0.02288670465350151]]\n",
      "0\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "1\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "2\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "3\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "4\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "5\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "6\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "7\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "8\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "9\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "10\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "11\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "12\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "13\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "14\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "15\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "16\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "17\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "18\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "19\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "20\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "21\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "22\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "23\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "24\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "25\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "26\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "27\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "28\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "29\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "30\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "31\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "32\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "33\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "34\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "35\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "36\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "37\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "38\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "39\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "40\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "41\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "42\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "43\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "44\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10\n",
      "45\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(list_words))\n",
    "print(max_score_index)\n",
    "print(list_words[len(list_words) - 1])\n",
    "\n",
    "for i in range(len(list_words)):\n",
    "    print(i)\n",
    "    print(type(list_words[i]))\n",
    "    word = list_words[i]\n",
    "    print(type(word))\n",
    "    print(len(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47, 10)\n",
      "(47, 10)\n"
     ]
    }
   ],
   "source": [
    "numpy_words = words.numpy()\n",
    "tensor_words =  tf.convert_to_tensor(numpy_words)\n",
    "print(numpy_words.shape)\n",
    "print(tensor_words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp-master-thesis)",
   "language": "python",
   "name": "nlp-master-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
