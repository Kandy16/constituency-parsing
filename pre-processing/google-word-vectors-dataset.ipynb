{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Binary format to text using gensim. Reduction of 300 dimensional did not work\n",
    "### total_vec argument also specified as 100 or 10 is not reducing the dimension.\n",
    "\n",
    "### Right now it is converted into text format sucessfuly with 300 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/kandy/hdd/master-thesis/constituency-parsing/datasets/\n"
     ]
    }
   ],
   "source": [
    "dirname = os.getcwd()\n",
    "dirname = os.path.dirname(dirname)\n",
    "dataset_path = os.path.join(dirname, 'datasets/')\n",
    "print(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(dataset_path+'GoogleNews-vectors-negative300.bin', binary=True)\n",
    "model.save_word2vec_format(dataset_path+'GoogleNews-vectors-negative300.txt', fvocab=dataset_path+'/google-vocab.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# failed attempts\n",
    "model.save_word2vec_format(dataset_path+'GoogleNews-vectors-negative100.txt', binary=False, total_vec=100)\n",
    "model.save_word2vec_format(dataset_path+'GoogleNews-vectors-negative10.txt', binary=False, total_vec=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read_csv chunking and using specific columns are also not working\n",
    "### skip rows worked smoothly\n",
    "### chunking - memory consumption was less. But later on it raised up and came down (could not understand). When about to store the chunked data , memory got overflowed in the middle\n",
    "### using specific columns did not reduce memory consumption (reading 11 and 301 columns are the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0         1         2         3         4         5         6         7   \\\n",
      "0  </s>  0.001129 -0.000896  0.000319  0.001534  0.001106 -0.001404 -0.000031   \n",
      "1    in  0.070312  0.086914  0.087891  0.062500  0.069336 -0.108887 -0.081543   \n",
      "2   for -0.011780 -0.047363  0.044678  0.063477 -0.018188 -0.063965 -0.001312   \n",
      "3  that -0.015747 -0.028320  0.083496  0.050293 -0.110352  0.031738 -0.014221   \n",
      "4    is  0.007050 -0.073242  0.171875  0.022583 -0.132812  0.198242  0.112793   \n",
      "\n",
      "         8         9         10  \n",
      "0 -0.000420 -0.000576  0.001076  \n",
      "1 -0.154297  0.020752  0.131836  \n",
      "2 -0.072266  0.064453  0.086426  \n",
      "3 -0.089844  0.117676  0.118164  \n",
      "4 -0.107910  0.071777  0.020874  \n"
     ]
    }
   ],
   "source": [
    "googleVectors =  pd.read_csv(dataset_path+'/GoogleNews-vectors-negative300.txt', sep=' ', header=None, skiprows=[0],\n",
    "                            usecols=[0,1,2,3,4,5,6,7,8,9,10])\n",
    "\n",
    "print(googleVectors.head())\n",
    "\n",
    "print(googleVectors.shape) #(2168995, 11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2168995, 11)\n"
     ]
    }
   ],
   "source": [
    "toy_google_vectors = googleVectors.iloc[:, 0:11]\n",
    "print(toy_google_vectors.shape)\n",
    "toy_google_vectors.to_csv(dataset_path+'GoogleNews-vectors-negative10.txt',header=False, index=False,sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader =  pd.read_csv(dataset_path+'GoogleNews-vectors-negative300.txt', sep=' ', header=None, skiprows=[0],\n",
    "                            usecols=[0,1,2,3,4,5,6,7,8,9,10], chunksize=50000)\n",
    "\n",
    "j=0\n",
    "for chunk in reader:\n",
    "    print(chunk.shape)\n",
    "    j = j+1\n",
    "    chunk.to_csv(dataset_path+'GoogleNews-vectors-negative10_'+str(j)+'.txt',header=False, index=False,sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use basic File reading and writing. Skipped lines uncessary\n",
    "#### split the lines and take only first 11 dimensional vector (word and 10 dimensions)\n",
    "#### Done for 11 dimensions. After that need to do it for 101 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "with open(dataset_path+'GoogleNews-vectors-negative300.txt', 'r') as read:\n",
    "    with open(dataset_path+'GoogleNews-vectors-negative10.txt', 'w') as write:\n",
    "        contents = read.readlines(5)\n",
    "        while(len(contents) != 0):\n",
    "            contents = read.readlines(30000)\n",
    "            contents = [content.split()[0:11] for content in contents]\n",
    "            contents = [' '.join(content)+'\\n' for content in contents]\n",
    "            write.writelines(contents)\n",
    "            #i = i + 1\n",
    "            #if (i == 10000):\n",
    "            #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2168995, 11)\n"
     ]
    }
   ],
   "source": [
    "google_mini = pd.read_csv(dataset_path+'GoogleNews-vectors-negative10.txt', sep =' ', header=None, prefix='col_')\n",
    "#print(google_10.head())\n",
    "print(google_mini.shape)\n",
    "print(google_mini.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lower(entity):\n",
    "    value = str(entity)\n",
    "    return value.lower()\n",
    "google_mini['col_0'] = google_mini['col_0'].apply(convert_to_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_mini.drop_duplicates(subset='col_0', keep='last', inplace=True)\n",
    "print(google_mini.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_mini.to_csv(dataset_path+'GoogleNews-vectors-negative10.txt', sep=' ', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1965717, 11)\n",
      "Index(['col_0', 'col_1', 'col_2', 'col_3', 'col_4', 'col_5', 'col_6', 'col_7',\n",
      "       'col_8', 'col_9', 'col_10'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "google_mini = pd.read_csv(dataset_path+'GoogleNews-vectors-negative10.txt', sep =' ', header=None, prefix='col_')\n",
    "#print(google_10.head())\n",
    "print(google_mini.shape)\n",
    "print(google_mini.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serialize google corpus and dictionaries to convert word into int and back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1965716\n"
     ]
    }
   ],
   "source": [
    "googleCorpus = set()\n",
    "google_mini['col_0'].str.lower().apply(googleCorpus.add)\n",
    "print(len(googleCorpus))\n",
    "\n",
    "google_corpus_word_to_int = {}\n",
    "google_corpus_int_to_word = {}\n",
    "for word in googleCorpus:\n",
    "    temp = len(google_corpus_word_to_int)\n",
    "    google_corpus_word_to_int[word] = temp\n",
    "    google_corpus_int_to_word[temp] = word\n",
    "#print(imdb_corpus_word_to_int)\n",
    "#print(imdb_corpus_int_to_word)\n",
    "\n",
    "temp = len(google_corpus_word_to_int)\n",
    "\n",
    "\n",
    "outfile = dataset_path+'google_word_corpus.pic'\n",
    "with open(outfile, 'wb') as pickle_file:\n",
    "    pickle.dump([googleCorpus,google_corpus_word_to_int, google_corpus_int_to_word], pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp-master-thesis)",
   "language": "python",
   "name": "nlp-master-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
