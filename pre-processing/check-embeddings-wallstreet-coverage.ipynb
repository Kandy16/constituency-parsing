{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk as nltk\n",
    "\n",
    "import re\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/kandy/hdd/master-thesis/constituency-parsing/datasets/\n"
     ]
    }
   ],
   "source": [
    "dirname = os.getcwd()\n",
    "dirname = os.path.dirname(dirname)\n",
    "dataset_path = os.path.join(dirname, 'datasets/')\n",
    "print(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the corpus and extract the trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WSJ.txt']\n",
      "<class 'nltk.corpus.reader.bracket_parse.BracketParseCorpusReader'>\n"
     ]
    }
   ],
   "source": [
    "reader = nltk.corpus.BracketParseCorpusReader(dataset_path,'WSJ.txt')\n",
    "print(reader.fileids())\n",
    "print(type(reader))\n",
    "\n",
    "## reads the file and converts each line into a tree\n",
    "trees = reader.parsed_sents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44377\n"
     ]
    }
   ],
   "source": [
    "word_tokens = set()\n",
    "for tree in trees:\n",
    "    for word in tree.leaves():\n",
    "        word_tokens.add(word.lower())\n",
    "        #word_tokens.add(word.lower()) # lower casing resulted in 44377 word tokens\n",
    "print(len(word_tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Google word2vec vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1965716\n"
     ]
    }
   ],
   "source": [
    "#googleVocab = pd.read_csv(dataset_path+'google-vocab.txt', sep=' ', header=None)\n",
    "#googleVocab.columns = ['word', 'index']\n",
    "#googleVocab.head()\n",
    "\n",
    "outfile = dataset_path +'google_word_corpus.pic'\n",
    "\n",
    "with open(outfile, 'rb') as pickle_file:    \n",
    "    googleCorpus, google_corpus_word_to_int, google_corpus_int_to_word = pickle.load(pickle_file)\n",
    "    \n",
    "google_vocab_set = set(googleCorpus)\n",
    "print(len(google_vocab_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearly 30% of words are missing. Casing does not make substantial difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13278\n",
      "29.920904973296977 % of words are missing!!!\n"
     ]
    }
   ],
   "source": [
    "diff_wj_google = word_tokens.difference(google_vocab_set)\n",
    "print(len(diff_wj_google))\n",
    "#print(diff_wj_google)\n",
    "print(float(len(diff_wj_google)) * 100 / float(len(word_tokens)), '% of words are missing!!!' )\n",
    "# lower casing resulted a difference of 13278 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Glove vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1783088\n"
     ]
    }
   ],
   "source": [
    "outfile = dataset_path +'glove_word_corpus.pic'\n",
    "\n",
    "with open(outfile, 'rb') as pickle_file:    \n",
    "    gloveCorpus, glove_corpus_word_to_int, glove_corpus_int_to_word = pickle.load(pickle_file)\n",
    "    \n",
    "gloveCorpus = set(gloveCorpus)\n",
    "gloveCorpus = list(gloveCorpus)\n",
    "gloveCorpus = [word.lower() for word in gloveCorpus]\n",
    "gloveCorpus = set(gloveCorpus)\n",
    "print(len(gloveCorpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Glove nearly 80% of words are missing. Good to go with google word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36692\n",
      "82.68247064921017 % of words are missing!!!\n"
     ]
    }
   ],
   "source": [
    "diff_wj_glove = word_tokens.difference(gloveCorpus)\n",
    "print(len(diff_wj_glove))\n",
    "#print(diff_wj_google)\n",
    "print(float(len(diff_wj_glove)) * 100 / float(len(word_tokens)), '% of words are missing!!!' )\n",
    "# lower casing resulted a difference of 13278 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp-master-thesis)",
   "language": "python",
   "name": "nlp-master-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
