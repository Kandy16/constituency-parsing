{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of a sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/kandy/hdd/master-thesis/constituency-parsing/datasets/\n"
     ]
    }
   ],
   "source": [
    "dirname = os.getcwd()\n",
    "dirname = os.path.dirname(dirname)\n",
    "dataset_path = os.path.join(dirname, 'datasets/')\n",
    "print(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1783088, 11)\n",
      "    0        1         2         3         4         5         6       7   \\\n",
      "0    ,  0.18378 -0.121230 -0.119870  0.015227 -0.191210 -0.066074 -2.9876   \n",
      "1  the -0.20838 -0.149320 -0.017528 -0.028432 -0.060104 -0.264600 -4.1445   \n",
      "2    .  0.10876  0.002244  0.222130 -0.121020 -0.048959  0.018135 -3.8174   \n",
      "3  and -0.09611 -0.257880 -0.358600 -0.328870  0.579500 -0.517740 -4.1582   \n",
      "4   to -0.24837 -0.454610  0.039227 -0.284220 -0.031852  0.263550 -4.6323   \n",
      "\n",
      "         8         9         10  \n",
      "0  0.807950  0.067338 -0.131840  \n",
      "1  0.629320  0.336720 -0.433950  \n",
      "2 -0.032631 -0.625940 -0.518980  \n",
      "3 -0.113710 -0.108480 -0.488850  \n",
      "4  0.013890 -0.539280 -0.084454  \n"
     ]
    }
   ],
   "source": [
    "gloveSet = pd.read_csv(dataset_path+'glove.42B.10d.txt', sep=' ', header=None)\n",
    "print(gloveSet.shape)\n",
    "print(gloveSet.head())\n",
    "\n",
    "gloveWords = gloveSet.iloc[:,0:1]\n",
    "gloveVectors = gloveSet.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1783088\n"
     ]
    }
   ],
   "source": [
    "gloveCorpus = set()\n",
    "gloveWords.iloc[:,0].str.lower().apply(gloveCorpus.add)\n",
    "print(len(gloveCorpus))\n",
    "\n",
    "glove_corpus_word_to_int = {}\n",
    "glove_corpus_int_to_word = {}\n",
    "for word in gloveCorpus:\n",
    "    temp = len(glove_corpus_word_to_int)\n",
    "    glove_corpus_word_to_int[word] = temp\n",
    "    glove_corpus_int_to_word[temp] = word\n",
    "#print(imdb_corpus_word_to_int)\n",
    "#print(imdb_corpus_int_to_word)\n",
    "\n",
    "UNK = '<unk>'\n",
    "temp = len(glove_corpus_word_to_int)\n",
    "\n",
    "\n",
    "outfile = glovePath +'glove_word_corpus.pic'\n",
    "with open(outfile, 'wb') as pickle_file:\n",
    "    pickle.dump([gloveCorpus,glove_corpus_word_to_int, glove_corpus_int_to_word], pickle_file)\n",
    "    \n",
    "#with open(outfile, 'rb') as pickle_file:    \n",
    "#   loaded_data1 = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(748, 2)\n",
      "                                            comments  sentiment\n",
      "0  A very, very, very slow-moving, aimless movie ...          0\n",
      "1  Not sure who was more lost - the flat characte...          0\n",
      "2  Attempting artiness with black & white and cle...          0\n",
      "3       Very little music or anything to speak of.            0\n",
      "4  The best scene in the movie was when Gerardo i...          1\n"
     ]
    }
   ],
   "source": [
    "sentiment_dataset = pd.read_csv(dataset_path+'imdb_labelled.txt', sep='\\t', header=None)\n",
    "sentiment_dataset.rename({0:'comments',1:'sentiment'}, axis='columns', inplace=True) \n",
    "print(sentiment_dataset.shape)\n",
    "print(sentiment_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1243201, 1127477, 1127477, 1127477, 1127477, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1127477, 1127477, 1127477, 1127477, 1127477, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1127477, 305266, 1127477, 1127477, 1127477, 1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1127477, 1127477, 1127477, 1127477, 1127477, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[682986, 1127477, 1127477, 893553, 682986, 112...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comments  sentiment\n",
       "0  [1243201, 1127477, 1127477, 1127477, 1127477, ...          0\n",
       "1  [1127477, 1127477, 1127477, 1127477, 1127477, ...          0\n",
       "2  [1127477, 305266, 1127477, 1127477, 1127477, 1...          0\n",
       "3  [1127477, 1127477, 1127477, 1127477, 1127477, ...          0\n",
       "4  [682986, 1127477, 1127477, 893553, 682986, 112...          1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_imdb_corpus_into_int(sentence):\n",
    "    words = sentence.lower().split()\n",
    "    words = [word if word in gloveCorpus else UNK for word in words]\n",
    "    words_to_num = [glove_corpus_word_to_int[word] for word in words]\n",
    "    return words_to_num\n",
    "\n",
    "sentiment_dataset_num = sentiment_dataset.copy()\n",
    "sentiment_dataset_num['comments'] = sentiment_dataset['comments'].apply(convert_imdb_corpus_into_int)\n",
    "sentiment_dataset_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train, imdb_test = sklearn.model_selection.train_test_split(sentiment_dataset_num, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              comments  sentiment\n",
      "522  [798015, 1127477, 900296, 1127477, 1127477, 11...          0\n",
      "233  [1127477, 1127477, 1127477, 1243201, 1127477, ...          0\n",
      "328  [1127477, 1127477, 1127477, 1127477, 1127477, ...          0\n",
      "272       [1127477, 682986, 1518213, 1127477, 1682641]          0\n",
      "465  [1127477, 1127477, 1127477, 1127477, 1127477, ...          1\n",
      "(598, 2)\n",
      "                                              comments  sentiment\n",
      "510  [1127477, 1127477, 1264798, 1127477, 1264798, ...          0\n",
      "575  [682986, 1127477, 893553, 1127477, 1127477, 89...          1\n",
      "335  [1127477, 1127477, 1127477, 1450383, 1127477, ...          1\n",
      "642  [1127477, 1127477, 1127477, 1264798, 682986, 1...          0\n",
      "474  [1127477, 1127477, 1127477, 1243201, 1127477, ...          1\n",
      "(150, 2)\n"
     ]
    }
   ],
   "source": [
    "print(imdb_train.head())\n",
    "print(imdb_train.shape)\n",
    "\n",
    "print(imdb_test.head())\n",
    "print(imdb_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14602\n",
      "11261\n"
     ]
    }
   ],
   "source": [
    "unkCount = 0\n",
    "totalCount = 0\n",
    "def count_UNK(token_list):\n",
    "    global unkCount, totalCount\n",
    "    match = glove_corpus_word_to_int[UNK]\n",
    "    #print(token_list)\n",
    "    #print(match)\n",
    "    unkCount += token_list.count(match)\n",
    "    totalCount += len(token_list)\n",
    "    \n",
    "sentiment_dataset_num['comments'].apply(count_UNK)\n",
    "print(totalCount)\n",
    "print(unkCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = tfe.Variable(name='embeddings', validate_shape= gloveVectors.shape, \n",
    "                          initial_value=gloveVectors.values, \n",
    "                          dtype=tf.float32, trainable=False)\n",
    "w = tfe.Variable(name='w', validate_shape=(gloveVectors.shape[1], 1), \n",
    "                 initial_value=0.01 * tf.random_normal(shape=(gloveVectors.shape[1], 1)),\n",
    "                 dtype=tf.float32)\n",
    "b = tfe.Variable(name='b', validate_shape=(1, 1),\n",
    "                 initial_value=0.01 * tf.random_normal(shape=(1, 1)),\n",
    "                 dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\n",
      "0.7435041\n",
      "Epoch  2\n",
      "Epoch  3\n",
      "Epoch  4\n",
      "Epoch  5\n",
      "Epoch  6\n",
      "Epoch  7\n",
      "Epoch  8\n",
      "Epoch  9\n",
      "Epoch  10\n",
      "Epoch  11\n",
      "Epoch  12\n",
      "Epoch  13\n",
      "Epoch  14\n",
      "Epoch  15\n",
      "Epoch  16\n",
      "Epoch  17\n",
      "Epoch  18\n",
      "Epoch  19\n",
      "Epoch  20\n",
      "Epoch  21\n",
      "Epoch  22\n",
      "Epoch  23\n",
      "Epoch  24\n",
      "Epoch  25\n",
      "Epoch  26\n",
      "Epoch  27\n",
      "Epoch  28\n",
      "Epoch  29\n",
      "Epoch  30\n",
      "Epoch  31\n",
      "Epoch  32\n",
      "Epoch  33\n",
      "Epoch  34\n",
      "Epoch  35\n",
      "Epoch  36\n",
      "Epoch  37\n",
      "Epoch  38\n",
      "Epoch  39\n",
      "Epoch  40\n",
      "Epoch  41\n",
      "Epoch  42\n",
      "Epoch  43\n",
      "Epoch  44\n",
      "Epoch  45\n",
      "Epoch  46\n",
      "Epoch  47\n",
      "Epoch  48\n",
      "Epoch  49\n",
      "Epoch  50\n",
      "Epoch  51\n",
      "Epoch  52\n",
      "Epoch  53\n",
      "Epoch  54\n",
      "Epoch  55\n",
      "Epoch  56\n",
      "Epoch  57\n",
      "Epoch  58\n",
      "Epoch  59\n",
      "Epoch  60\n",
      "Epoch  61\n",
      "Epoch  62\n",
      "Epoch  63\n",
      "Epoch  64\n",
      "Epoch  65\n",
      "Epoch  66\n",
      "Epoch  67\n",
      "Epoch  68\n",
      "Epoch  69\n",
      "Epoch  70\n",
      "Epoch  71\n",
      "Epoch  72\n",
      "Epoch  73\n",
      "Epoch  74\n",
      "Epoch  75\n",
      "Epoch  76\n",
      "Epoch  77\n",
      "Epoch  78\n",
      "Epoch  79\n",
      "Epoch  80\n",
      "Epoch  81\n",
      "Epoch  82\n",
      "Epoch  83\n",
      "Epoch  84\n",
      "Epoch  85\n",
      "Epoch  86\n",
      "Epoch  87\n",
      "Epoch  88\n",
      "Epoch  89\n",
      "Epoch  90\n",
      "Epoch  91\n",
      "Epoch  92\n",
      "Epoch  93\n",
      "Epoch  94\n",
      "Epoch  95\n",
      "Epoch  96\n",
      "Epoch  97\n",
      "Epoch  98\n",
      "Epoch  99\n",
      "Epoch  100\n",
      "Epoch  101\n",
      "0.7442389\n",
      "Epoch  102\n",
      "Epoch  103\n",
      "Epoch  104\n",
      "Epoch  105\n",
      "Epoch  106\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-5e7baa1151ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimdb_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0;31m#print(grads)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             optimizer.apply_gradients(zip(grads, [w,b]),\n",
      "\u001b[0;32m~/anaconda3/envs/nlp-master-thesis/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients)\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m         output_gradients=output_gradients)\n\u001b[0m\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp-master-thesis/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients)\u001b[0m\n\u001b[1;32m     62\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m       \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       output_gradients)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/nlp-master-thesis/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp-master-thesis/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_SelectGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1115\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m   \u001b[0mzeros\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m   return (None, array_ops.where(c, grad, zeros), array_ops.where(\n\u001b[0m\u001b[1;32m   1118\u001b[0m       c, zeros, grad))\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp-master-thesis/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mwhere\u001b[0;34m(condition, x, y, name)\u001b[0m\n\u001b[1;32m   2622\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2623\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2624\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2625\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2626\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y must both be non-None or both be None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp-master-thesis/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(condition, x, y, name)\u001b[0m\n\u001b[1;32m   7008\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[1;32m   7009\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Select\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7010\u001b[0;31m         _ctx._post_execution_callbacks, condition, x, y)\n\u001b[0m\u001b[1;32m   7011\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7012\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "epoch = 1000\n",
    "for i in range(epoch):\n",
    "    for j in range(imdb_train.shape[0]):\n",
    "        with tf.GradientTape() as tape:\n",
    "            words = tf.nn.embedding_lookup(embeddings, imdb_train.iat[j,0])\n",
    "            #print(words)\n",
    "            cbow_words_avg = tf.math.reduce_mean(words, axis=0, keepdims=True)\n",
    "            #print(cbow_words_avg)\n",
    "            z = tf.matmul(cbow_words_avg, w) + b\n",
    "            y_predict = tf.sigmoid(z)\n",
    "            #print(y_predict)\n",
    "            loss = tf.losses.sigmoid_cross_entropy(tf.constant(imdb_train.iat[j,1], shape=(1,1)), y_predict)\n",
    "           \n",
    "            grads = tape.gradient(loss, [w,b])\n",
    "            #print(grads)\n",
    "            optimizer.apply_gradients(zip(grads, [w,b]),\n",
    "                            global_step=tf.train.get_or_create_global_step())\n",
    "    if(i % 100 == 0):\n",
    "        print('Epoch ', i+1)\n",
    "        print(loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable(1.0)\n",
    "assign = a.assign(2.0)\n",
    "with tf.control_dependencies([assign]):\n",
    "  b = a.read_value()\n",
    "with tf.control_dependencies([b]):\n",
    "  other_assign = a.assign(3.0)\n",
    "with tf.control_dependencies([other_assign]):\n",
    "  # Will print 2.0 because the value was read before other_assign ran. If\n",
    "  # `a` was a tf.Variable instead, 2.0 or 3.0 could be printed.\n",
    "  tf.Print(b, [b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=9309656, shape=(), dtype=float32, numpy=2.0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp-master-thesis)",
   "language": "python",
   "name": "nlp-master-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
