{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk as nltk\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enabling eager execution and check versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n",
      "2.1.6-tf\n"
     ]
    }
   ],
   "source": [
    "tf.enable_eager_execution()\n",
    "print(tf.VERSION)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Glove vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/kandy/hdd/master-thesis/constituency-parsing/datasets/\n"
     ]
    }
   ],
   "source": [
    "dirname = os.getcwd()\n",
    "dirname = os.path.dirname(dirname)\n",
    "dataset_path = os.path.join(dirname, 'datasets/')\n",
    "print(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1783088, 11)\n",
      "    0        1         2         3         4         5         6       7   \\\n",
      "0    ,  0.18378 -0.121230 -0.119870  0.015227 -0.191210 -0.066074 -2.9876   \n",
      "1  the -0.20838 -0.149320 -0.017528 -0.028432 -0.060104 -0.264600 -4.1445   \n",
      "2    .  0.10876  0.002244  0.222130 -0.121020 -0.048959  0.018135 -3.8174   \n",
      "3  and -0.09611 -0.257880 -0.358600 -0.328870  0.579500 -0.517740 -4.1582   \n",
      "4   to -0.24837 -0.454610  0.039227 -0.284220 -0.031852  0.263550 -4.6323   \n",
      "\n",
      "         8         9         10  \n",
      "0  0.807950  0.067338 -0.131840  \n",
      "1  0.629320  0.336720 -0.433950  \n",
      "2 -0.032631 -0.625940 -0.518980  \n",
      "3 -0.113710 -0.108480 -0.488850  \n",
      "4  0.013890 -0.539280 -0.084454  \n"
     ]
    }
   ],
   "source": [
    "UNK = '<unk>'\n",
    "\n",
    "outfile = dataset_path +'glove_word_corpus.pic'\n",
    "\n",
    "with open(outfile, 'rb') as pickle_file:    \n",
    "    gloveCorpus, glove_corpus_word_to_int, glove_corpus_int_to_word = pickle.load(pickle_file)\n",
    "\n",
    "gloveSet = pd.read_csv(glovePath+'glove.42B.10d.txt', sep=' ', header=None)\n",
    "print(gloveSet.shape)\n",
    "print(gloveSet.head())\n",
    "\n",
    "gloveWords = gloveSet.iloc[:,0:1]\n",
    "gloveVectors = gloveSet.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WSJ.txt']\n",
      "<class 'nltk.corpus.reader.bracket_parse.BracketParseCorpusReader'>\n",
      "No. of trees:  49208\n",
      "<class 'nltk.tree.Tree'>\n"
     ]
    }
   ],
   "source": [
    "## https://www.nltk.org/_modules/nltk/tree.html\n",
    "## above link contains the API and also some tutorials\n",
    "\n",
    "#reader = nltk.corpus.BracketParseCorpusReader('.','SWB-all-sentences-original-with-punctuation.MRG')\n",
    "reader = nltk.corpus.BracketParseCorpusReader(dataset_path,'WSJ.txt')\n",
    "print(reader.fileids())\n",
    "print(type(reader))\n",
    "\n",
    "## reads the file and converts each line into a tree\n",
    "trees = reader.parsed_sents()\n",
    "print('No. of trees: ', len(trees))\n",
    "print(type(trees[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>tree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Pierre, Vinken, ,, 61, years, old, ,, will, j...</td>\n",
       "      <td>[[[(NP (NNP Pierre) (NNP Vinken)), (, ,), (ADJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Mr., Vinken, is, chairman, of, Elsevier, N.V....</td>\n",
       "      <td>[[[(NNP Mr.), (NNP Vinken)], [(VBZ is), (NP-PR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Rudolph, Agnew, ,, 55, years, old, and, forme...</td>\n",
       "      <td>[[[(NP (NNP Rudolph) (NNP Agnew)), (, ,), (UCP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[A, form, of, asbestos, once, used, *, *, to, ...</td>\n",
       "      <td>[[[(NP-SBJ\\n  (NP (NP (DT A) (NN form)) (PP (I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The, asbestos, fiber, ,, crocidolite, ,, is, ...</td>\n",
       "      <td>[[[(NP-SBJ\\n  (NP (DT The) (NN asbestos) (NN f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  [Pierre, Vinken, ,, 61, years, old, ,, will, j...   \n",
       "1  [Mr., Vinken, is, chairman, of, Elsevier, N.V....   \n",
       "2  [Rudolph, Agnew, ,, 55, years, old, and, forme...   \n",
       "3  [A, form, of, asbestos, once, used, *, *, to, ...   \n",
       "4  [The, asbestos, fiber, ,, crocidolite, ,, is, ...   \n",
       "\n",
       "                                                tree  \n",
       "0  [[[(NP (NNP Pierre) (NNP Vinken)), (, ,), (ADJ...  \n",
       "1  [[[(NNP Mr.), (NNP Vinken)], [(VBZ is), (NP-PR...  \n",
       "2  [[[(NP (NNP Rudolph) (NNP Agnew)), (, ,), (UCP...  \n",
       "3  [[[(NP-SBJ\\n  (NP (NP (DT A) (NN form)) (PP (I...  \n",
       "4  [[[(NP-SBJ\\n  (NP (DT The) (NN asbestos) (NN f...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treesDict = [{'sentence': tree.leaves(), 'tree':tree} for tree in trees]\n",
    "treeDataframe = pd.DataFrame(data=treesDict, columns=['sentence', 'tree'])\n",
    "treeDataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>tree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[349063, 349063, 1380359, 349063, 349063, 3490...</td>\n",
       "      <td>[[[(NP (NNP Pierre) (NNP Vinken)), (, ,), (ADJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[349063, 349063, 349063, 349063, 788117, 34906...</td>\n",
       "      <td>[[[(NNP Mr.), (NNP Vinken)], [(VBZ is), (NP-PR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[349063, 349063, 1380359, 349063, 349063, 3490...</td>\n",
       "      <td>[[[(NP (NNP Rudolph) (NNP Agnew)), (, ,), (UCP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[349063, 349063, 788117, 349063, 349063, 34906...</td>\n",
       "      <td>[[[(NP-SBJ\\n  (NP (NP (DT A) (NN form)) (PP (I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[349063, 349063, 349063, 1380359, 870463, 1380...</td>\n",
       "      <td>[[[(NP-SBJ\\n  (NP (DT The) (NN asbestos) (NN f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  [349063, 349063, 1380359, 349063, 349063, 3490...   \n",
       "1  [349063, 349063, 349063, 349063, 788117, 34906...   \n",
       "2  [349063, 349063, 1380359, 349063, 349063, 3490...   \n",
       "3  [349063, 349063, 788117, 349063, 349063, 34906...   \n",
       "4  [349063, 349063, 349063, 1380359, 870463, 1380...   \n",
       "\n",
       "                                                tree  \n",
       "0  [[[(NP (NNP Pierre) (NNP Vinken)), (, ,), (ADJ...  \n",
       "1  [[[(NNP Mr.), (NNP Vinken)], [(VBZ is), (NP-PR...  \n",
       "2  [[[(NP (NNP Rudolph) (NNP Agnew)), (, ,), (UCP...  \n",
       "3  [[[(NP-SBJ\\n  (NP (NP (DT A) (NN form)) (PP (I...  \n",
       "4  [[[(NP-SBJ\\n  (NP (DT The) (NN asbestos) (NN f...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_imdb_corpus_into_int(words):\n",
    "    words = [word if word in gloveCorpus else UNK for word in words]\n",
    "    words_to_num = [glove_corpus_word_to_int[word] for word in words]\n",
    "    return words_to_num\n",
    "\n",
    "treeDataframe_num = treeDataframe.copy()\n",
    "treeDataframe_num['sentence'] = treeDataframe_num['sentence'].apply(convert_imdb_corpus_into_int)\n",
    "treeDataframe_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49, 2)\n",
      "(49159, 2)\n"
     ]
    }
   ],
   "source": [
    "treeDF_train, treeDF_test = sklearn.model_selection.train_test_split(treeDataframe_num, test_size=0.999)\n",
    "print(treeDF_train.shape)\n",
    "print(treeDF_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SIZE = 10\n",
    "embeddings = tfe.Variable(name='embeddings', validate_shape= gloveVectors.shape, \n",
    "                          initial_value=gloveVectors.values, \n",
    "                          dtype=tf.float32, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class constituencyParsing(tf.keras.Model):\n",
    "    def __init__(self, input_size):\n",
    "        super(constituencyParsing, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=1, activation=tf.sigmoid, input_shape=(input_size,))\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        words = inputs\n",
    "        #words matrix - unstack\n",
    "        words_unstack = tf.unstack(words)\n",
    "        words_len = len(words_unstack)\n",
    "        pred_score_list = []\n",
    "        total_loss_list = []\n",
    "        #loop until all the words are merged together\n",
    "        while(words_len > 1):\n",
    "            #compute scores for the list of word combinations\n",
    "            # for each word combination compute the score of it\n",
    "            scores = np.zeros(shape=(words_len-1, 1))\n",
    "            for k in range(words_len - 1):\n",
    "                words_concat = tf.concat([words_unstack[k], words_unstack[k+1]], axis=0)\n",
    "                #reshape the tensor to be a matrix with 1 row rather than vector\n",
    "                words_concat = tf.reshape(words_concat, shape=(1, words_concat.shape[0]))\n",
    "                # matrix computation and activation\n",
    "                z = tf.matmul(words_concat, w) + b\n",
    "                state_vec = tf.tanh(z)\n",
    "                score = tf.matmul(state_vec, w_score) + b_score\n",
    "                scores[k] = score\n",
    "\n",
    "            #print(scores)\n",
    "            #compare the scores and pick the maximum one. \n",
    "            max_score_index = np.argmax(scores) \n",
    "            pred_score_list.append(scores[max_score_index])\n",
    "\n",
    "            # remove the words which is used to combine and replace with combined state vector\n",
    "            words_unstack.pop(max_score_index+1)\n",
    "            words_unstack.pop(max_score_index)\n",
    "            # statevector needs to be reshaped as matrix to update\n",
    "            state_vec_vector = tf.reshape(state_vec, shape = [state_vec.shape[1]])\n",
    "            words_unstack.insert(max_score_index, state_vec_vector)\n",
    "            words_len = len(words_unstack)\n",
    "\n",
    "        # get the actual tree - convert it to chomsky normal form, and compute the score\n",
    "        act_score_list = []\n",
    "        tree = treeDF_train.iat[j,1]\n",
    "        tree.chomsky_normal_form()\n",
    "        compute_score_for_tree(tree[0], [w,b,w_score,b_score], embeddings, act_score_list)\n",
    "\n",
    "        # compute the total actual and predicted score. use the loss function as absolute difference\n",
    "        # the above is done for each training data and the loss are accumulated\n",
    "        total_act_score = tf.reduce_sum(tf.stack(act_score_list))\n",
    "        total_pred_score = tf.reduce_sum(tf.stack(pred_score_list))\n",
    "        loss = tf.losses.absolute_difference(total_act_score,  total_pred_score)\n",
    "        total_loss_list.append(loss)\n",
    "        #loss = tf.losses.sigmoid_cross_entropy(tf.constant(imdb_train.iat[j,1], shape=(1,1)), y_predict)\n",
    "\n",
    "        #compute the average losses accompanying all training data\n",
    "        # compute the gradients and apply them on variables\n",
    "        total_loss = tf.reduce_mean(tf.stack(total_loss_list))\n",
    "        grads = tape.gradient(total_loss, [w,b,w_score,b_score])\n",
    "        print(w[0])\n",
    "        grad_op = optimizer.apply_gradients(zip(grads, [w,b,w_score,b_score]), \n",
    "                                  global_step=tf.train.get_or_create_global_step())\n",
    "        print(w[0])\n",
    "        #maintain the history of variables, losses and gradients\n",
    "        variables_history.append([np.copy(w.numpy()), np.copy(b.numpy()),np.copy(w_score.numpy()),np.copy(b_score.numpy())])\n",
    "        loss_history.append(np.copy(total_loss.numpy()))\n",
    "        grad_history.append([np.copy(grad.numpy()) for grad in grads])\n",
    "        #print(tf.train.get_or_create_global_step().numpy(),total_loss.numpy())\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    def compute_score_for_tree(tree, weights, embeddings, scores):\n",
    "        if(type(tree[0]) == type('a string')):\n",
    "            #print(tree.label() + ' : ' + tree[0])\n",
    "            word = tree[0].lower()\n",
    "            if(word not in glove_corpus_word_to_int):\n",
    "                word = '<unk>'\n",
    "            word_vector = tf.nn.embedding_lookup(embeddings, glove_corpus_word_to_int[word])\n",
    "            word_vector = tf.reshape(word_vector, shape=(1, word_vector.shape[0]))\n",
    "            return word_vector\n",
    "            #print('depth is reached !!!')\n",
    "            #return\n",
    "\n",
    "        #for i in range(len(tree)):\n",
    "        #    print('Inside tree : '+ tree[i].label())\n",
    "        #    compute_score_for_tree(tree[i], weights, embeddings)\n",
    "\n",
    "\n",
    "\n",
    "        left = compute_score_for_tree(tree[0], weights, embeddings, scores)\n",
    "        if(len(tree) !=2):\n",
    "            return left\n",
    "\n",
    "        right = compute_score_for_tree(tree[1], weights, embeddings, scores)\n",
    "        words_concat = tf.concat([left, right], axis=0)\n",
    "        #print(words_concat.shape)\n",
    "        #print(left.shape, right.shape)\n",
    "        words_concat = tf.reshape(words_concat, shape=(1, left.shape[1] + right.shape[1]))\n",
    "        #print(words_concat)\n",
    "        z = tf.matmul(words_concat, weights[0]) + weights[1]\n",
    "        state_vec = tf.tanh(z)\n",
    "\n",
    "        #print(state_vec)\n",
    "        score = tf.matmul(state_vec, weights[2]) + weights[3]\n",
    "        scores.append(score)\n",
    "        return state_vec\n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp-master-thesis)",
   "language": "python",
   "name": "nlp-master-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
